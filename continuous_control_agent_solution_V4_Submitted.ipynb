{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils\n",
    "from torch.distributions import Normal\n",
    "\n",
    "import csv\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>CUDA</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Ornstein-Uhlenbeck Process</h1>\n",
    "Adding time-correlated noise to the actions taken by the deterministic policy<br>\n",
    "<a href=\"https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\">wiki</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUNoise(object):\n",
    "    def __init__(self, action_space_size, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "\n",
    "        self.action_dim   = action_space_size\n",
    "        self.low          = -1\n",
    "        self.high         = 1\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # same\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "\n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "\n",
    "        return self.state\n",
    "\n",
    "    def get_action(self, actions, t=0):\n",
    "\n",
    "        ou_state = self.evolve_state()\n",
    "\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "\n",
    "        return np.clip(actions + ou_state, self.low, self.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Actor Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Actor\n",
    "#\n",
    "# receives a state and returns a value for each possible action in that state (allows for continuous action spaces)\n",
    "#\n",
    "class ActorNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.linear3 = nn.Linear(hidden_size, num_actions)\n",
    "\n",
    "        # random initialisation of last layer weights and bias\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        x = self.linear1(state)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # for continuous action spaces we need to use a tanh for each possible action component (e.g. torque1=0.5, torque2=-0.1, velocity1=1.0, velocity2=0.3)\n",
    "        x = self.linear3(x)\n",
    "        x = F.tanh(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def get_actions(self, states):\n",
    "\n",
    "        # returns a new tensor with a dimension of size one inserted at the specified position: puts the tensor inside a tensor. returns a copy in cpu/gpu memory.\n",
    "        states  = torch.FloatTensor(states).unsqueeze(0).to(device)\n",
    "\n",
    "        # performs a forward pass and retrieves the best values for all action components\n",
    "        actions = self.forward(states)\n",
    "\n",
    "        # creates a new tensor detached from the graph that created it, inside cpu memory and returns it as a numpy array.\n",
    "        return actions.detach().cpu().numpy()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Critic Network</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Critic\n",
    "#\n",
    "# receives a state and actions for that state from the actor, and returns its evaluation of it\n",
    "#\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, num_inputs, num_actions, hidden_size, init_w=3e-3):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
    "\n",
    "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "        self.linear3 = nn.Linear(hidden_size, 1)\n",
    "\n",
    "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "\n",
    "        x = torch.cat([states, actions], 1)\n",
    "\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.linear3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Replay Buffer</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# ReplayBuffer\n",
    "#\n",
    "# Stores experiences from the 20-agent environments which are used during the learning process\n",
    "#\n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        # how does this work?\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Agents Handler</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# AgentsHandler\n",
    "#\n",
    "# Manages the learning process of the 20-agent environment instances\n",
    "#\n",
    "class AgentsHandler:\n",
    "\n",
    "    def __init__(self, num_agents, state_dim, action_dim, replay_buffer_size=1000000, batch_size=128, hidden_dim=256, critic_lr=1e-3, actor_lr=1e-4):\n",
    "\n",
    "        # stores the number of agents\n",
    "        self.num_agents = num_agents\n",
    "        \n",
    "        # instantiates a new shared replay buffer\n",
    "        self.replay_buffer_size = replay_buffer_size\n",
    "        self.replay_buffer = ReplayBuffer(self.replay_buffer_size)\n",
    "\n",
    "        # size of batch of experiments to use when training the agents' actor and critic nextworks\n",
    "        self.batch_size  = batch_size\n",
    "\n",
    "        # steps to take before learning again\n",
    "        self.steps_to_learning = 1\n",
    "        \n",
    "        # local critic network\n",
    "        self.critic_net  = CriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        # local actor network\n",
    "        self.actor_net = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        # target critic network\n",
    "        self.critic_target_net  = CriticNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        # target actor network\n",
    "        self.actor_target_net = ActorNetwork(state_dim, action_dim, hidden_dim).to(device)\n",
    "\n",
    "        # copies the initial parameters from the local network to the target network\n",
    "        for target_param, param in zip(self.critic_target_net.parameters(), self.critic_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        for target_param, param in zip(self.actor_target_net.parameters(), self.actor_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "\n",
    "        # learning rates\n",
    "        self.critic_lr  = critic_lr\n",
    "        self.actor_lr = actor_lr\n",
    "\n",
    "        # optimisers\n",
    "        self.critic_optimizer  = optim.Adam(self.critic_net.parameters(),  lr = self.critic_lr)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_net.parameters(), lr = self.actor_lr)\n",
    "\n",
    "        # loss function for the critic network\n",
    "        #self.critic_loss = nn.MSELoss()\n",
    "        self.critic_loss = nn.SmoothL1Loss()\n",
    "\n",
    "        # number of steps to take before copying the local networks' parameters to the target networks\n",
    "        self.actor_target_network_parameter_update_steps = 30\n",
    "        self.critic_target_network_parameter_update_steps = 30\n",
    "\n",
    "        # noise function for actions\n",
    "        self.ou_noise = OUNoise(action_dim)\n",
    "\n",
    "    def reset_noise(self):\n",
    "\n",
    "        # resets the noise function\n",
    "        self.ou_noise.reset()\n",
    "\n",
    "    def act(self, states, step):\n",
    "\n",
    "        # gets an action from the actor's network\n",
    "        self.actor_net.eval()\n",
    "        actions = self.actor_net.get_actions(states)\n",
    "        self.actor_net.train()\n",
    "\n",
    "        # if training then adds some noise to the agent decided actions to encourage exploration\n",
    "        if training_mode == True:\n",
    "            actions = self.ou_noise.get_action(actions, step)\n",
    "\n",
    "        # removes the top dimension\n",
    "        actions = np.squeeze(actions, axis=0)\n",
    "\n",
    "        # returns the chosen action\n",
    "        return actions\n",
    "\n",
    "    def step(self, step, states, actions, rewards, next_states, dones):\n",
    "\n",
    "        # stores the current experience in the SHARED replay buffer\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.replay_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        # learn from the accumulated experiences in the replay buffer\n",
    "        self.learn(step)\n",
    "\n",
    "    def learn(self, step, gamma=0.99, expected_values_min_clamp=-np.inf, expected_values_max_clamp=np.inf, tau=1e-2):\n",
    "\n",
    "        # if the replay buffer has enough experiences then we can start learning from them in batch\n",
    "        if len(self.replay_buffer) > self.batch_size:\n",
    "            \n",
    "            # only learn every n steps: for stability reasons\n",
    "            if step % self.steps_to_learning == 0:\n",
    "\n",
    "                # collects a batch of experiences to train the actor and critic\n",
    "                experiences = self.replay_buffer.sample(self.batch_size)\n",
    "\n",
    "                # collects a batch of experiences to train the actor and critic\n",
    "                states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "                # converts the numpy arrays to tensors and prepare the tensors to run on a cpu/gpu device\n",
    "                states      = torch.FloatTensor(states).to(device)\n",
    "                next_states = torch.FloatTensor(next_states).to(device)\n",
    "                actions     = torch.FloatTensor(actions).to(device)\n",
    "                rewards     = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n",
    "                dones       = torch.FloatTensor(np.float32(dones)).unsqueeze(1).to(device)\n",
    "\n",
    "                #\n",
    "                # Actor's training\n",
    "                #\n",
    "\n",
    "                # gets the best actions from the actor for the sampled states within this experience batch using the current policy\n",
    "                actor_actions = self.actor_net(states)\n",
    "\n",
    "                # uses the critic to evaluate the actions indicated by the actor for the sampled states\n",
    "                critic_evaluations = self.critic_net(states, actor_actions)\n",
    "\n",
    "                # computes the actor's mean loss for the entire batch\n",
    "                actor_loss    = -critic_evaluations.mean()\n",
    "\n",
    "                # gets the next action for the next state using the actor's target network\n",
    "                next_actions    = self.actor_target_net(next_states)\n",
    "\n",
    "                # computes the Q-value for the next state and action using the critic's target network\n",
    "                target_values   = self.critic_target_net(next_states, next_actions.detach())\n",
    "\n",
    "                # computes the final expected value\n",
    "                expected_values = rewards + (1.0 - dones) * gamma * target_values\n",
    "\n",
    "                # clamps the expected value -> we should instead normalise the expected values, right? clamping simply cuts them.\n",
    "                expected_values = torch.clamp(expected_values, expected_values_min_clamp, expected_values_max_clamp)\n",
    "\n",
    "                #\n",
    "                # Critic's training\n",
    "                #\n",
    "\n",
    "                # Gets the value of the current state and taken action\n",
    "                critic_state_action_values = self.critic_net(states, actions)\n",
    "\n",
    "                # computes the loss between the current value obtained by the critic's local network and the expected value -> MSE/Huber Loss\n",
    "                critic_loss = self.critic_loss(critic_state_action_values, expected_values.detach())\n",
    "                \n",
    "                #\n",
    "                # Optimisation of both networks\n",
    "                #\n",
    "\n",
    "                # takes a step on the policy optimiser and updates the policy network's parameters\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor_net.parameters(), 1)\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "                # takes a step on the critic optimiser and updates the policy network's parameters\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic_net.parameters(), 1)\n",
    "                self.critic_optimizer.step()\n",
    "                \n",
    "                #\n",
    "                # Target networks soft-update (every step or every C steps)\n",
    "                #\n",
    "\n",
    "                # updates the critic's target network\n",
    "                if step % self.critic_target_network_parameter_update_steps == 0:\n",
    "\n",
    "                    for target_param, param in zip(self.critic_target_net.parameters(), self.critic_net.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            target_param.data * (1.0 - tau) + param.data * tau\n",
    "                        )\n",
    "\n",
    "                # updates the actor's target network\n",
    "                if step % self.actor_target_network_parameter_update_steps == 0:\n",
    "\n",
    "                    for target_param, param in zip(self.actor_target_net.parameters(), self.actor_net.parameters()):\n",
    "                        target_param.data.copy_(\n",
    "                            target_param.data * (1.0 - tau) + param.data * tau\n",
    "                        )\n",
    "\n",
    "                    \n",
    "    def save_agent_model(self, model_path):\n",
    "\n",
    "        # saves the actor model parameters for later usage in test or production\n",
    "        torch.save(self.actor_net.state_dict(), model_path)\n",
    "\n",
    "        \n",
    "    def load_agent_model(self, model_path):\n",
    "        \n",
    "        # loads a saved model for inference purposes\n",
    "        self.actor_net.load_state_dict(torch.load(model_path))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Auxiliary Functions</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(i_episode, scores):\n",
    "    clear_output(True)\n",
    "\n",
    "    plt.figure(figsize=(30,5))\n",
    "\n",
    "    plt.subplot(131)\n",
    "\n",
    "    plt.title('Episode %s - Last episode average score: %s' % (i_episode, scores[-1]))\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.plot(np.arange(1, len(scores) + 1), scores)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Training Routine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(number_of_episodes=1000, max_steps_per_episode=1000, average_score_episodes=100, print_every_episodes=10, print_every_steps=100):\n",
    "\n",
    "    # scores bookkeeping\n",
    "    last_100_episodes_scores = deque(maxlen = average_score_episodes)\n",
    "    scores_log = []\n",
    "\n",
    "    # instantiates the agent\n",
    "    agents_handler = AgentsHandler(num_agents, state_size, action_size)\n",
    "\n",
    "    # starts counter in seconds\n",
    "    time_start = time.time()\n",
    "\n",
    "    for i_episode in range(1, number_of_episodes + 1):\n",
    "\n",
    "        # reset the environment\n",
    "        env_info = env.reset(train_mode = True)[brain_name]\n",
    "\n",
    "        # get the current state of the environment (for each agent)\n",
    "        states = env_info.vector_observations               # (num_agents x 33)\n",
    "\n",
    "        # resets the noise function\n",
    "        agents_handler.reset_noise()\n",
    "\n",
    "        # initialize the score (for each agent)\n",
    "        episode_total_scores = np.zeros(num_agents)\n",
    "\n",
    "        for step in range(1, max_steps_per_episode + 1):\n",
    "\n",
    "            # gets an action from the actor's network\n",
    "            actions = agents_handler.act(states, step)      # (num_agents x 4)\n",
    "\n",
    "            # takes a step in the environment\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "\n",
    "            #  gets rewards, next_states and dones\n",
    "            rewards = env_info.rewards                      # (num_agents x 1)\n",
    "            next_states = env_info.vector_observations      # (num_agents x 33)\n",
    "            dones = env_info.local_done                     # (num_agents x 1)\n",
    "\n",
    "            # takes a step in the agents\n",
    "            agents_handler.step(step, states, actions, rewards, next_states, dones)\n",
    "\n",
    "            # sets the next state\n",
    "            states = next_states\n",
    "\n",
    "            # updates the current episode's score (num_agents x 1)\n",
    "            episode_total_scores += env_info.rewards\n",
    "            \n",
    "            # echoes the episode's current total reward\n",
    "            if step % print_every_steps == 0:\n",
    "                print(\"\\nEpisode {} step {} average reward so far -> {}\".format(i_episode, step, np.mean(episode_total_scores)))\n",
    "\n",
    "            if np.any(dones):\n",
    "                break\n",
    "\n",
    "        # calculates the episode's average score across all agents\n",
    "        episode_average_score = np.mean(episode_total_scores)\n",
    "                \n",
    "        # adds this episode average score to the complete scores log\n",
    "        scores_log.append(episode_average_score)\n",
    "        \n",
    "        # adds this episode's total score to the last 100 scores average deque\n",
    "        last_100_episodes_scores.append(episode_average_score)\n",
    "        \n",
    "        # plots the latest 100 episodes average reward\n",
    "        if i_episode % print_every_episodes == 0:\n",
    "            plot_scores(i_episode, scores_log)\n",
    "            \n",
    "        # checks if the environment is solved\n",
    "        if np.mean(last_100_episodes_scores) > 30:\n",
    "            print(\"Environment resolved in {} episodes!\".format(i_episode))\n",
    "            break\n",
    "\n",
    "    # stop the timer\n",
    "    time_end = time.time()\n",
    "\n",
    "    # prints the duration in minutes\n",
    "    print((time_end-time_start) / 60)\n",
    "    \n",
    "    # gets the current date/time\n",
    "    now = datetime.datetime.now()\n",
    "        \n",
    "    # builds a new model name\n",
    "    model_path = 'agent_model_trained_' + now.strftime(\"%Y%m%d_%H%M%S\") + '.pth'\n",
    "    \n",
    "    # saves the learned model\n",
    "    agents_handler.save_agent_model(model_path)\n",
    "\n",
    "    # echoes model path\n",
    "    print(\"Model saved at: \", model_path)\n",
    "    \n",
    "    # closes the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Testing Routine</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(number_of_episodes=10, max_steps_per_episode=1000, average_score_episodes=100, print_every_episodes=1, print_every_steps=100):\n",
    "    \n",
    "    # scores bookkeeping\n",
    "    scores_log = []\n",
    "\n",
    "    # instantiates the agent in test mode\n",
    "    agents_handler = AgentsHandler(num_agents, state_size, action_size)\n",
    "    \n",
    "    # loads the model into the critic network\n",
    "    agents_handler.load_agent_model(\"agent_model_trained_20181019_123626.pth\")\n",
    "\n",
    "    for i_episode in range(1, number_of_episodes + 1):\n",
    "\n",
    "        # reset the environment\n",
    "        env_info = env.reset(train_mode = False)[brain_name]\n",
    "\n",
    "        # get the current state of the environment (for each agent)\n",
    "        states = env_info.vector_observations               # (num_agents x 33)\n",
    "\n",
    "        # initialize the score (for each agent)\n",
    "        episode_total_scores = np.zeros(num_agents)\n",
    "\n",
    "        for step in range(1, max_steps_per_episode + 1):\n",
    "\n",
    "            # gets an action from the actor's network\n",
    "            actions = agents_handler.act(states, step)      # (num_agents x 4)\n",
    "            \n",
    "            # takes a step in the environment\n",
    "            env_info = env.step(actions)[brain_name]\n",
    "\n",
    "            #  gets rewards, next_states and dones\n",
    "            rewards = env_info.rewards                      # (num_agents x 1)\n",
    "            next_states = env_info.vector_observations      # (num_agents x 33)\n",
    "            dones = env_info.local_done                     # (num_agents x 1)\n",
    "\n",
    "            # sets the next state\n",
    "            states = next_states\n",
    "            \n",
    "            # updates the current episode's score (num_agents x 1)\n",
    "            episode_total_scores += env_info.rewards\n",
    "            \n",
    "            # echoes the episode's current total reward\n",
    "            if step % print_every_steps == 0:\n",
    "                print(\"\\nEpisode {} step {} average reward so far -> {}\".format(i_episode, step, np.mean(episode_total_scores)))\n",
    "\n",
    "        # calculates the episode's average score across all agents\n",
    "        episode_average_score = np.mean(episode_total_scores)\n",
    "                \n",
    "        # adds this episode average score to the complete scores log\n",
    "        scores_log.append(episode_average_score)\n",
    "        \n",
    "        # plots the latest 100 episodes average reward\n",
    "        if i_episode % print_every_episodes == 0:\n",
    "            plot_scores(i_episode, scores_log)\n",
    "            \n",
    "    # closes the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Control Routines for Training and Testing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_mode = False\n",
    "\n",
    "if training_mode == True:\n",
    "    \n",
    "    # echoes current task\n",
    "    print(\"Training started... :-)\")\n",
    "    \n",
    "    # instantiates a new environment in test mode\n",
    "    env = UnityEnvironment(file_name='Reacher20.app', no_graphics=True, seed=0)\n",
    "\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "    print('The state for the first agent looks like:', states[0])\n",
    "    \n",
    "    # starts our tests\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_mode = True\n",
    "\n",
    "if testing_mode == True:\n",
    "    \n",
    "    # echoes current task\n",
    "    print(\"Testing started... :-)\")\n",
    "    \n",
    "    # instantiates a new environment in test mode\n",
    "    env = UnityEnvironment(file_name='Reacher20.app', no_graphics=False, seed=10)\n",
    "\n",
    "    # get the default brain\n",
    "    brain_name = env.brain_names[0]\n",
    "    brain = env.brains[brain_name]\n",
    "\n",
    "    # reset the environment\n",
    "    env_info = env.reset(train_mode=False)[brain_name]\n",
    "\n",
    "    # number of agents\n",
    "    num_agents = len(env_info.agents)\n",
    "    print('Number of agents:', num_agents)\n",
    "\n",
    "    # size of each action\n",
    "    action_size = brain.vector_action_space_size\n",
    "    print('Size of each action:', action_size)\n",
    "\n",
    "    # examine the state space\n",
    "    states = env_info.vector_observations\n",
    "    state_size = states.shape[1]\n",
    "\n",
    "    print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "    print('The state for the first agent looks like:', states[0])\n",
    "    \n",
    "    # starts our tests\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
